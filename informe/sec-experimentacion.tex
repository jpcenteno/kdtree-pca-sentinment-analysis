\section{Experimentación}%
\label{sec:experimentacion}

% TL;DR; Intro experimentación.
En esta sección se incluye la experimentación llevada a cabo sobre nuestro
modelo de clasificador.
%
% TL;DR xq sklearn:

\subsection{Metodología}%
\label{sub:metodologia}
\subsubsection{Dependencia de factores}

Cuando consideramos una métrica de precisión como función objetivo de todos estos parámetros
no resulta manejable optimizar en un único paso de manera multivariada (son muchos)
ni tampoco analizar parámetro a parámetro dado que cada uno afecta como variable de manera
interpendiente a la función objetivo no pudiendo aislar un \textit{orden topológico}
de parámetros para optimizar de manera independiente.
En respuesta a esto se plantea un esquema de experimentación basado en aproximar
un orden de optimización de parámetros que sea relativamente parecido a un orden
topológico fijando algunos de ellos en valores intuitivos para eliminar dependencias
y poder seguir con las optimizaciones de a pares o por separado según sea conveniente
de manera de respetar lo máximo posible las dependencias.
Aunque solamente realizamos una iteración como prueba de concepto, este esquema
permite poder ir actualizando los valores que se fijan de manera iterativa incremental
mejorando paso a paso la función objetivo. El esquema en cuestión es:
\begin{itemize}
    \item buscar tolerancia de convergencia para método de la potencia (desempata candidatos con PCA con algunos parámetros fijos)
    \item buscar el k óptimo para kNN sin PCA en función de alguna vectorización fija
    \item buscar k y alfa óptimos para kNN con  PCA guiando el rango de k según el paso anterior nuevamente con vectorización fija
    \item buscar parámetros de vectorización óptimos para kNN según los parámetros anteriores
\end{itemize}


\subsection{Convergencia de método de la potencia}%
\label{sub:pm}
\input{sec-experimentacion-pm.tex}

\subsection{Impacto del tamaño del set de entrenamiento}%
\label{sub:exp_training_set}
\input{sec-experimentacion-tamano-set-entrenamiento.tex}

\subsection{Optimización de $k$ para \knn{} sin PCA}%
\label{sub:knn_sin_pca}

Para buscar una $k$ cantidad de vecinos que optimice \knn{} usamos nuevamente el dataset de \textit{imdb\_small}, iteramos un $k$ de 1 a 3000 con saltos de a 100 al principio y luego analizamos la región al rededor del máximo que encontramos.

Originalmente pensábamos arrancar en un rango mas adelantado dado que para $k$ pequeños sobre muestras grandes se puede correr riesgos de overfitting por puntos ``ruidosos" que estén demasiado pegados ganandole en la votación a la verdadera clase, pero como $k$ pequeños no son caros de computar los consideramos en la experimentación de todos modos.

Elegir $k$ grande tiene el problema de que los elementos de la clase de un punto quizás están todos en su vecindad inmediata y a medida que vamos buscando vecinos en zonas más alejadas vamos tendiendo a clasificar peor el punto: una clase muy densa pero rodeada de otra mas dispersa y sobrerrepresentada será peor catalogada cuanto más grande sea $k$. Si además $k$ es lo suficientemente grande (aproximadamente superando la mitad de la población total) la proporción de vecinos más cercanos se parece cada vez más y más a la proporción muestral de cada clase sobre el total, lo cual no aporta nada de información. Por lo tanto decidimos iterar solamente hasta la mitad.

Tendría sentido con este análisis que acabamos de hacer encontrar los mejores resultados en un punto intermedio en la magnitud de $k$.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{./img/knn.png}
\centering
\caption{Progresión de accuracy score de KNN en función de la cantidad de vecinos.\label{fig:knn_acc}}

\end{figure}

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{./img/knn2.png}
\centering
\caption{Progresión de accuracy score de KNN en función de la cantidad de vecinos en la zona del máximo.\label{fig:knn_acc_2}}

\end{figure}

Como podemos apreciar en la figura \ref{fig:knn_acc}, se cumple nuestra predicción de que el accuracy score empeora conforme la cantidad de vecinos se vuelve muy grande o muy pequeña. Focalizando en el pico del máximo local más grande en la figura \ref{fig:knn_acc_2} se ve que alcanzamos el máximo accuracy score en $0.685$ para $k=1826$ que se sitúa muy cerca de la mitad del intervalo que elegimos, interesantemente superando el $0.681116$ para $k=1801$ que nos había arrojado Scikit-learn para una experimentación similar en primera instancia. Nos resulta complicado comprender por qué desciende el accuracy score en los primeros valores, especulamos con que los problemas de ruido y sesgo requieran valores de $k$ bajos pero mayores a los mínimos para que empiecen a aparecer. Otro motivo sea que si el espacio de los vectores no esté separado de manera clara existan zonas con más ruido que en el resto y que se vayan acumulando outliers de clases difusas hasta que se sale de los mismos.

\subsection{Optimización de $k$ y $\alpha$ para \knn{} con PCA}%
\label{sub:alpha_k_knn_pca}
\input{sec-experimentacion-hyperparams.tex}

\section{Experimentación}%
\label{sec:experimentacion}

% TL;DR; Intro experimentación.
En esta sección se incluye la experimentación llevada a cabo sobre nuestro
modelo de clasificador.
%
% TL;DR xq sklearn:
Por restricciones de tiempo, todos los experimentos fueron corridos sobre la
implementación de \textit{Scikit Learn}.

\subsection{Metodología}%
\label{sub:metodologia}

\subsection{Convergencia de método de la potencia}%
\label{sub:pm}
\input{sec-experimentacion-pm.tex}

\subsection{Impacto del tamaño del set de entrenamiento}%
\label{sub:exp_training_set}

% TL;DR; Como se diseño el experimento
%
Fueron tomadas mediciones sobre el comportamiento del \textit{accuracy} al
reducir el tamaño del set de entrenamiento para \knn{} con PCA.\@ Para ello se
redujo el tamaño muestral, tomando distintas sub-muestras al azar, respetando
una proporción pareja entre reseñas con calificación positiva y negativa.
% k y alpha
Se mantuvieron constantes $k$ y $\alpha$. El experimento se repitió para
distintas configuraciones de los mismos.

% acc vs N; k fijo; distintos alpha
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{img/exp_subsampling_k_fijo}
    \caption{\textit{Accuracy} obtenido al reducir el conjunto de
    entrenamiento.  Para valor fijo de $k=100$.}%
    \label{fig:subsampling_k_fijo}
\end{figure}

% acc vs N; alpha fijo; distintos k
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{img/exp_subsampling_alpha_fijo}
    \caption{\textit{Accuracy} obtenido al reducir el conjunto de
    entrenamiento.  Para valor fijo de $\alpha=100$.}%
    \label{fig:subsampling_alpha_fijo}
\end{figure}

% TL;DR; mas data => mas accuracy.
%
Puede observarse en las figuras~\ref{fig:subsampling_k_fijo}
y~\ref{fig:subsampling_alpha_fijo} que el \textit{accuracy} incrementa al
incrementar el tamaño muestral.  Por otro lado, la calidad del modelo crece
cada vez en menor medida, por lo que se cree que existe un punto a partir del
cual carece de sentido incrementar el set de entrenamiento. Como el rendimiento
del modelo sigue siendo bajo, creemos que la máxima cantidad de instancias de
entrenamiento disponible, $N = 6225$, esta por debajo del tamaño óptimo.

% TL;DR; El tamaño óptimo parece ser independiente de k y alpha.
%
Se observó el mismo comportamiento de la curva para todas las configuraciones
de $k$ y $\alpha$, por lo que se cree, el tamaño muestral óptimo es
independiente de los hiperparámetros a escoger.

% TL;DR; Se observa mayor varianza para N bajo.
%
A su vez se observa que la varianza en el \textit{accuracy} es mayor para
tamaños muestrales chicos. Esto es porque las reseñas conocidas por el modelo
pueden ser, con mayor probabilidad, poco representativas la totalidad de las
reseñas.

\subsection{Optimización de $k$ para \knn{} sin PCA}%
\label{sub:knn_sin_pca}

Para buscar una $k$ cantidad de vecinos que optimice \knn{} usamos nuevamente el dataset de \textit{imdb\_small}, iteramos un $k$ de 1 a 3000 con saltos de a 20.

Originalmente pensábamos arrancar en un rango mas adelantado dado que para $k$ pequeños sobre muestras grandes se puede correr riesgos de overfitting por puntos ``ruidosos" que estén demasiado pegados ganandole en la votación a la verdadera clase, pero como $k$ pequeños no son caros de computar los consideramos en la experimentación de todos modos.

Elegir $k$ grande tiene el problema de que los elementos de la clase de un punto quizás están todos en su vecindad inmediata y a medida que vamos buscando vecinos en zonas más alejadas vamos tendiendo a clasificar peor el punto: una clase muy densa pero rodeada de otra mas dispersa y sobrerrepresentada será peor catalogada cuanto más grande sea $k$. Si además $k$ es lo suficientemente grande (aproximadamente superando la mitad de la población total) la proporción de vecinos más cercanos se parece cada vez más y más a la proporción muestral de cada clase sobre el total, lo cual no aporta nada de información. Por lo tanto decidimos iterar solamente hasta la mitad.

Tendría sentido con este análisis que acabamos de hacer encontrar los mejores resultados en un punto intermedio en la magnitud de $k$.

\begin{figure}[h]
\includegraphics[width=\textwidth]{./img/knn.png}
\centering
\caption{Progresión de accuracy score de KNN en función de la cantidad de vecinos.\label{fig:knn_acc}}

\end{figure}

Como podemos apreciar en la figura \ref{fig:knn_acc}, se cumple nuestra predicción de que el accuracy score empeora conforme la cantidad de vecinos se vuelve muy grande o muy pequeña. Alcanzamos el máximo accuracy score en 0.681116 para $k=1801$ que se sitúa muy cerca de la mitad del intervalo que elegimos. Nos resulta complicado comprender por qué desciende el accuracy score en los primeros valores, especulamos con que los problemas de ruido y sesgo requieran valores de $k$ bajos pero mayores a los mínimos para que empiecen a aparecer. Otro motivo sea que si el espacio de los vectores no esté separado de manera clara existan zonas con más ruido que en el resto y que se vayan acumulando outliers de clases difusas hasta que se sale de los mismos.

\subsection{Optimización de $k$ y $\alpha$ para \knn{} con PCA}%
\label{sub:alpha_k_knn_pca}
\input{sec-experimentacion-hyperparams.tex}

\section{Introducción Teórica}%
\label{sec:introduccion_teorica}

El análisis del lenguaje natural y machine learning son dos areas
importantes en el análisis de datos que experimentaron en los últimos
años un avance sinérgico. El análisis de lenguaje tiene varias
aplicaciones, desde mejorar las interfaces humano-máquina hasta la
detección automática de pedofilia en las redes sociales.

En el presente trabajo se efectúa un análisis de sentimiento sobre
reseñas de películas usando un algoritmo de machine learning que
clasifica de forma binariamente cada reseña.

\subsection{Análisis de Sentimiento}%
\label{sub:analisis_de_sentimiento}

\subsection{Bag of Words}%
\label{sub:bag_of_words}

\textit{Bag of Words} es un sistema de representación utilizado en
procesamiento del lenguaje natural. Cada documento pasa a ser representado por
su multiconjunto de palabras. Se pierde el orden original de las palabras en el
texto. Se utiliza en problemas donde el vocabulario del documento es suficiente
para estimar el sentimiento subyacente.

% FIXME tirar un ejemplo ilustrativo


\subsection{k Nearest Neighbors}%
\label{sub:k_nearest_neighbors}

% knn in a nutshell:
El algoritmo \textit{$k$ Nearest Neighbors}, \knn{} es un clasificador
supervisado que estima la categoría de una instancia basándose en su vecindad
con instancias ya conocidas.
% Para que tipos de problemas sirve
Se desempeña mejor en problemas donde la vecindad espacial entre
observaciones es un factor importante en la clasificación de instancias.

% Como funciona el algoritmo
Para una instancia de entrada $x$, se calcula su distancia con todo el set de
entrenamiento. La clasificación estimada resultante es la categoría modal (voto
mayoritario) entre los $k$ vecinos mas cercanos a $x$ en el set de
entrenamiento.

% Cuales son los parámetros
Influye en \knn{} la elección del $k$ en el rango $1 \leq k \leq N$, así como
la elección de la norma a utilizar para medir distancia.
% Que pasa con `k` alto.
Para un valor de $k$ alto, la estimación puede verse afectada por la categoría
modal en el conjunto de entrenamiento.
% Que pasa con `k` chico
Para una eleción de $k$ chico, la clasificación puede verse afectada por la
presencia de \textit{outliers} en su cercanía.
% FIXME hablar de distintas normas.

% FIXME hablar de algo que presente por que podemos necesitar PCA.

\subsection{Principal Component Analysis}%
\label{sub:principal_component_analysis}

\textit{Principal Component Analysis}, PCA, es un algoritmo de reducción de
dimensionalidad que transforma los datos de entrada en un espacio de
\textit{features} correlacionadas a un conjunto de valores cuyas
\textit{features} no guarden correlación entre sí.


Si del conjunto de vectores $\{v_1,..v_n \} \subset \mathds{R}^m$ que
conseguimos mediante Bag of Words tomamos el vector de medias
muestrales $\mu = \sum \frac{v_i}{n} \in \mathds{R}^n $ podemos
conseguir la matriz $X \in \mathds{R}^{m\times n}$ con filas
$\frac{v_i-\mu}{\sqrt{n-1}}$ podemos construir su \textit{matriz de
  covarianza} $M=\frac{X^t X}{n-1}$ de cuya descomposición SVD, como
para toda matriz $X$ vale que $X^t X$ es simétrica con autovalores
reales y base ortonormal de autovectores también reales, conseguimos
una base de autovectores ordenada decrecientemente de acuerdo a la
varianza interna de cada componente (que se asocia a qué tan grande es
su autovalor) y con nula covarianza entre sí de modo que un cambio de
base con los primeros autovectores pase la matriz a una base con menos
ruido entre componentes.

Esto también nos permite recortar los últimos vectores de la base dado
que son los que menos información proveen (además de ahorrar tiempo y
espacio al considerar menos componentes espaciales en $kNN$), que
llamermos una \textit{reducción de dimensionalidad} a $\alpha$
cantidad de vectores.

\subsection{Método de la potencia}%
\label{sub:pm}
Se trata de un método iterativo que permite hallar los autovalores dominantes de
una matriz dada (aquellos cuya norma supera al resto) y sus autovectores
asociados. O sea conseguimos un $\lambda_1$ tal que:

$$ |\lambda_1| > |\lambda_2| \geq ... \geq |\lambda_n| $$

Para todos los autovalores $\lambda_i$ en una matriz de $n\times n$.

La idea es usar este método para conseguir los autovectores de las $\alpha$
componentes más significativas descriptas en \ref{sub:principal_component_analysis}. Al tratarse
de un método iterativo que converge asintóticamente tendremos que implementar distintos criterios de parada para dejar de iterar.

Usamos la variante del método de la potencia que se presenta para matrices simétricas (como ya vimos, la matriz de covarianzas cumple tal propiedad) vista en la sección 9.3 del libro de Burden \cite{Burden}.
